{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iLQjIsmtkATa"},"outputs":[],"source":["# Clone the extensions into the web UI's extensions directory\n","!git clone https://github.com/Mikubill/sd-webui-controlnet.git /content/drive/MyDrive/photo_gen_cloud/sd-webui-controlnet\n","!git clone https://github.com/huchenlei/sd-webui-openpose-editor.git /content/drive/MyDrive/photo_gen_cloud/sd-webui-openpose-editor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1818,"status":"ok","timestamp":1749002534482,"user":{"displayName":"Siu Hin Fong","userId":"02185117597770121470"},"user_tz":-480},"id":"9QrVcth5rDpC","outputId":"b31f6e64-0993-4a72-971f-9441a81a038b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into '/content/drive/MyDrive/photo_gen_cloud/sd-webui-adetailer'...\n","remote: Enumerating objects: 3075, done.\u001b[K\n","remote: Counting objects: 100% (970/970), done.\u001b[K\n","remote: Compressing objects: 100% (121/121), done.\u001b[K\n","remote: Total 3075 (delta 916), reused 850 (delta 849), pack-reused 2105 (from 3)\u001b[K\n","Receiving objects: 100% (3075/3075), 562.70 KiB | 6.54 MiB/s, done.\n","Resolving deltas: 100% (1795/1795), done.\n"]}],"source":["!git clone https://github.com/Bing-su/adetailer.git /content/drive/MyDrive/photo_gen_cloud/sd-webui-adetailer"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0t6SVa0sr9UM"},"outputs":[],"source":["!pip uninstall mediapipe\n","!pip install mediapipe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9817,"status":"ok","timestamp":1749003304059,"user":{"displayName":"Siu Hin Fong","userId":"02185117597770121470"},"user_tz":-480},"id":"PGB3cn1muFFV","outputId":"f1810be3-7d9a-4e5e-a12d-b5f028a3d3be"},"outputs":[{"name":"stdout","output_type":"stream","text":["FaceDetection initialized successfully!\n"]}],"source":["import mediapipe as mp\n","mp_face_detection = mp.solutions.face_detection\n","face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n","print(\"FaceDetection initialized successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vTrcNkbjaDk7"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'mediapipe'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-04cc92a2c497\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import torch\n","from diffusers import EulerAncestralDiscreteScheduler, StableDiffusionControlNetInpaintPipeline, ControlNetModel, DPMSolverMultistepScheduler, StableDiffusionInpaintPipeline\n","from PIL import Image, ImageFilter\n","import os\n","import mediapipe as mp\n","import numpy as np\n","\n","# Load the ControlNet model for OpenPose\n","controlnet = ControlNetModel.from_pretrained(\n","    \"lllyasviel/control_v11p_sd15_openpose\",\n","    torch_dtype=torch.float16\n",")\n","\n","# Load the model using .bin files\n","pipe = StableDiffusionInpaintPipeline.from_pretrained(\n","    \"runwayml/stable-diffusion-inpainting\",\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16,\n","    use_safetensors=False # Use .bin files instead of safetensors\n",")\n","pipe.to(\"cuda\")  # Move to GPU\n","pipe.enable_attention_slicing()  # Optimize for low VRAM\n","\n","# Set the scheduler to DPMSolverMultistepScheduler with Karras sigmas (Sampler: DPM++ 2M Karras)\n","pipe.scheduler = DPMSolverMultistepScheduler(\n","    beta_start=0.00085,\n","    beta_end=0.012,\n","    beta_schedule=\"scaled_linear\",\n","    num_train_timesteps=1000,\n","    use_karras_sigmas=True\n",")\n","\n","#pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n","\n","# Function to load images from local paths with error handling\n","def load_image(path):\n","    try:\n","        if not os.path.exists(path):\n","            raise FileNotFoundError(f\"Image not found at {path}\")\n","        image = Image.open(path).convert(\"RGB\").resize((512, 512))\n","        return image\n","    except Exception as e:\n","        print(f\"Error loading image: {e}\")\n","        raise\n","\n","# Local image paths in Colab\n","init_image_path = \"/content/drive/MyDrive/photo_gen_cloud/girl_in_tokyo_street.png\"\n","mask_image_path = \"/content/drive/MyDrive/photo_gen_cloud/masked_girl_in_tokyo_street.png\"\n","# pose_image_path =\n","\n","# Load images\n","init_image = load_image(init_image_path)\n","mask_image = load_image(mask_image_path).convert(\"L\")  # Ensure mask is grayscale\n","# pose_image =\n","\n","# Apply Gaussian blur to the mask (Mask blur: 0-64)\n","mask_image = mask_image.filter(ImageFilter.GaussianBlur(radius=32))\n","\n","# Check mask values (should be 0 and 255 for a proper binary mask)\n","print(f\"Mask min: {mask_image.getextrema()[0]}, max: {mask_image.getextrema()[1]}\")\n","\n","# Define prompts\n","prompt = \"Lady Gaga, highly realistic, facing forward, symmetrical face, detailed hands, delicate fingers, flawless skin, photorealistic portrait, cinematic lighting, sharp details, natural pose\"\n","negative_prompt = \"distorted, deformed, extra limbs, bad anatomy, disfigured, uneven eyes, asymmetrical face, blurry, low quality, unnatural proportions, extra fingers, fused hands, artifacts\"\n","\n","# Perform inpainting with settings matching AUTOMATIC1111\n","results = pipe(\n","    prompt=prompt,\n","    negative_prompt=negative_prompt,\n","    image=init_image,\n","    mask_image=mask_image,\n","    #control_image=pose_image,\n","    strength=0.9,  # Equivalent to Latent Noise for full generation\n","    guidance_scale=8,  # CFG Scale set to 12\n","    num_inference_steps=10,  # Increased steps for better quality\n","    num_images_per_prompt=4  # Batch size of 4\n",").images\n","\n","# Face detection and enhancement functions\n","mp_face_detection = mp.solutions.face_detection\n","\n","def detect_faces(image_pil, blur_radius=4):\n","    image_np = np.array(image_pil)\n","    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n","        results = face_detection.process(image_np)\n","        masks = []\n","        if results.detections:\n","            for detection in results.detections:\n","                bbox = detection.location_data.relative_bounding_box\n","                ih, iw, _ = image_np.shape\n","                xmin = int(bbox.xmin * iw)\n","                ymin = int(bbox.ymin * ih)\n","                width = int(bbox.width * iw)\n","                height = int(bbox.height * ih)\n","                mask = np.zeros((ih, iw), dtype=np.uint8)\n","                mask[ymin:ymin+height, xmin:xmin+width] = 255\n","                mask_pil = Image.fromarray(mask)\n","                mask_pil = mask_pil.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n","                masks.append(mask_pil)\n","        return masks\n","\n","def enhance_faces(image, pipe, prompt, strength=0.75, guidance_scale=7.5, num_inference_steps=25):\n","    face_masks = detect_faces(image)\n","    current_image = image\n","    for face_mask in face_masks:\n","        inpainted_image = pipe(\n","            prompt=prompt,\n","            image=current_image,\n","            mask_image=face_mask,\n","            strength=strength,\n","            guidance_scale=guidance_scale,\n","            num_inference_steps=num_inference_steps\n","        ).images[0]\n","        current_image = inpainted_image\n","    return current_image\n","\n","# Enhance faces in each result and save/display\n","for i, result in enumerate(results):\n","    enhanced_image = enhance_faces(result, pipe, prompt)\n","    enhanced_image.save(f\"enhanced_output_image_{i}.png\")\n","    display(enhanced_image)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMB9Ob1cR164K2nPDNSr/Vu","gpuType":"T4","mount_file_id":"1Mwmx5A6h0b-btWTNMZUGg7aN-qSMuKOl","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}