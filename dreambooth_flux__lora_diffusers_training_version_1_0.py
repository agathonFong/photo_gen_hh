# -*- coding: utf-8 -*-
"""dreambooth_flux _lora_diffusers_training_version_1_0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JZFMXU_7UKSOWCBkf8h5ip0eryXJRfTA

#  DreamBooth Flux LoRA Training
"""

# Commented out IPython magic to ensure Python compatibility.
# @title 1. Clean Environment and Install Correct Dependencies

# 1. Install all necessary libraries with compatible versions
print("Installing PyTorch, Accelerate, PEFT, and other core libraries...")
!pip install -qqq torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1
!pip install -qqq "accelerate>=0.31.0" # Install a known good version
!pip install -qqq "peft>=0.15.0"      # Install a known good version
!pip install -qqq transformers==4.43.3
!pip install -qqq xformers==0.0.28
!pip install -qqq bitsandbytes==0.43.1
!pip install -qqq huggingface_hub==0.29.0

# Pin Keras and h5py to stable versions for DeepFace compatibility
print("Installing compatible versions of Keras and DeepFace...")
!pip install -qqq "keras==2.15.0"
!pip install -qqq "h5py==3.10.0"
!pip install -qqq deepface

# 2. Verify the installations BEFORE proceeding
print("\n--- Verifying crucial library versions ---")
!pip show accelerate
!pip show peft
print("--------------------------------------\n")

# 3. Clone and install diffusers in editable mode
print("Cloning diffusers and installing from source...")
!rm -rf /content/diffusers # Remove old clone if it exists
!git clone https://github.com/huggingface/diffusers.git /content/diffusers
# %cd /content/diffusers
!pip install -e .
# %cd /content/

# 4. Download the training script
print("\nDownloading training script...")
!wget -O train_dreambooth_lora_flux.py https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_flux.py
!ls -l train_dreambooth_lora_flux.py # Verify download

print("\nSetup complete.")

# @title 2. Mount Google Drive and Set Paths
# This cell mounts your Google Drive where your dataset is stored and where
# your trained LoRA will be saved.

from google.colab import drive
import os

drive.mount('/content/drive')

# @markdown ### **User Configuration:**
# @markdown **Important:** Replace these with your actual paths and prompt!

# @markdown Path to your folder containing training images on Google Drive.
# @markdown Example: /content/drive/MyDrive/my_datasets/my_sks_dog
INSTANCE_DATA_DIR = "/content/drive/MyDrive/photo_gen_cloud/phtoto_and_masked/ckh_imgae" # @param {type:"string"}

# @markdown Path where the trained LoRA model will be saved.
# @markdown Example: /content/drive/MyDrive/flux_loras/my_sks_dog_lora
OUTPUT_DIR = "/content/drive/MyDrive/flux_models/my_character_lora" # @param {type:"string"}

# @markdown Your unique trigger word (instance prompt). Choose something rare!
# @markdown Example: "a photo of a sks dog" or "a painting in the mystyle art style"
INSTANCE_PROMPT = "a photo of a ckh male celebrity" # @param {type:"string"}

# @markdown Optional: Class prompt for regularization (e.g., "dog", "art style").
# @markdown Not strictly necessary for basic DreamBooth, but can help with generalization.
# @markdown If left empty, no class prompt will be used.
CLASS_PROMPT = "male celebrity" # @param {type:"string"}

# @markdown Create output directory if it doesn't exist
os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"Instance Data Directory: {INSTANCE_DATA_DIR}")
print(f"Output Directory: {OUTPUT_DIR}")
print(f"Instance Prompt: {INSTANCE_PROMPT}")
print(f"Class Prompt: {CLASS_PROMPT}")

if not os.path.exists(INSTANCE_DATA_DIR):
    print("\nWARNING: Instance Data Directory does not exist! Please check your path.")

# @title 3. Define Training Parameters

# @markdown ### **Model and Training Settings:**
MODEL_NAME = "black-forest-labs/FLUX.1-dev" # @param {type:"string"}
RESOLUTION = 1024 # @param [512, 1024] {type:"raw"}
TRAIN_BATCH_SIZE = 1 # @param {type:"integer"}
GRADIENT_ACCUMULATION_STEPS = 2 # @param {type:"integer"}
MIXED_PRECISION = "bf16" # @param ["no", "fp16", "bf16"]
LEARNING_RATE = "5e-5" # @param {type:"string"}
LR_SCHEDULER = "constant" # @param ["constant", "cosine", "linear", "polynomial"]
LR_WARMUP_STEPS = 0 # @param {type:"integer"}
MAX_TRAIN_STEPS = 1000 # @param {type:"integer"}

# @markdown ### **LoRA Specific Settings:**
# Note: --lora_rank is now --rank, and --lora_alpha is removed (defaults to rank).
LORA_RANK = 32 # @param [8, 16, 32, 64, 128] {type:"raw"}

# Note: --train_text_encoder_ti and --enable_t5_ti are replaced by a single flag.
TRAIN_TEXT_ENCODER = True # @param {type:"boolean"}

# @markdown ### **Memory Optimization and Other Flags:**
# Note: xformers and safetensors are now enabled by default and flags are removed.
GRADIENT_CHECKPOINTING = True # @param {type:"boolean"}
USE_8BIT_ADAM = True # @param {type:"boolean"}

# Note: --save_steps is now --checkpointing_steps.
SAVE_INTERVAL = 500 # @param {type:"integer"}
REPORT_TO_WANDB = False # @param {type:"boolean"}

# Construct the command for accelerate launch with CORRECTED arguments
TRAINING_COMMAND = f"""
accelerate launch \
    --mixed_precision="{MIXED_PRECISION}" \
    train_dreambooth_lora_flux.py \
    --pretrained_model_name_or_path="{MODEL_NAME}" \
    --instance_data_dir="{INSTANCE_DATA_DIR}" \
    --output_dir="{OUTPUT_DIR}" \
    --instance_prompt="{INSTANCE_PROMPT}" \
    --resolution={RESOLUTION} \
    --train_batch_size={TRAIN_BATCH_SIZE} \
    --gradient_accumulation_steps={GRADIENT_ACCUMULATION_STEPS} \
    --learning_rate={LEARNING_RATE} \
    --lr_scheduler={LR_SCHEDULER} \
    --lr_warmup_steps={LR_WARMUP_STEPS} \
    --max_train_steps={MAX_TRAIN_STEPS} \
    --rank={LORA_RANK} \
    --checkpointing_steps={SAVE_INTERVAL} \
    {"--gradient_checkpointing" if GRADIENT_CHECKPOINTING else ""} \
    {"--use_8bit_adam" if USE_8BIT_ADAM else ""} \
    {"--train_text_encoder" if TRAIN_TEXT_ENCODER else ""} \
    {"--report_to=wandb" if REPORT_TO_WANDB else ""}
"""

print("Corrected training command constructed. Review it below:")
print(TRAINING_COMMAND)

# @title 4. Download Training Script

!wget -O train_dreambooth_lora_flux.py https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_flux.py

!python train_dreambooth_lora_flux.py --help # check if the model function properly
print("Training script downloaded.")

# @title 5. Log in to Hugging Face

from huggingface_hub import login

login("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")

print("Login successful!")

# @title 6. Run the Training
# This is the main training execution cell.
# Monitor the output carefully for progress and any errors.

# If you enabled W&B logging, log in here.
if REPORT_TO_WANDB:
    import wandb
    wandb.login()

print("Starting training...")
!{TRAINING_COMMAND}

print("\nTraining completed! Your LoRA model should be saved in:")
print(OUTPUT_DIR)

# @title 7. Generate a Batch of Images

# --- Step 1: Install and Import Necessary Libraries ---
print("Installing required libraries...")
!pip install -q deepface

import torch
from diffusers import FluxPipeline
from huggingface_hub import login
import os
import glob
from PIL import Image
from deepface import DeepFace

# --- Step 2: Configuration ---
# Model and Path Configuration
MODEL_ID = "black-forest-labs/FLUX.1-dev"
LORA_PATH = "/content/drive/MyDrive/flux_models/my_character_lora"
ORIGINAL_IMAGES_DIR = "/content/drive/MyDrive/photo_gen_cloud/phtoto_and_masked/ckh_imgae"
GENERATED_IMAGES_DIR = "/content/drive/MyDrive/flux_models/my_character_lora/generated_images"
os.makedirs(GENERATED_IMAGES_DIR, exist_ok=True)

# Generation settings
prompt = "a high-resolution photo of a ckh male celebrity, looking at the camera, neutral expression"
num_images_to_generate = 2

# --- Step 3: Log in, Load Pipeline, and Generate Images ---

print("\nLogging in to Hugging Face...")
login('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')

print(f"Loading base model: {MODEL_ID}")
pipeline = FluxPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16)

print(f"Loading LoRA weights from: {LORA_PATH}")
pipeline.load_lora_weights(LORA_PATH)
pipeline.to("cuda")

print(f"\nGenerating {num_images_to_generate} images with prompt: '{prompt}'")
generated_images = pipeline(
    prompt=prompt,
    num_inference_steps=25,
    guidance_scale=7.0,
    num_images_per_prompt=num_images_to_generate
).images

# --- Step 4: Save the Generated Images ---

saved_image_paths = []
for i, image in enumerate(generated_images):
    image_path = os.path.join(GENERATED_IMAGES_DIR, f"generated_image_{i}.png")
    image.save(image_path)
    saved_image_paths.append(image_path)
    print(f"Saved: {image_path}")

# @title 8. Diagnostic for Image Comparison and Compare with DeepFace

# --- Step 1: Install and Import Necessary Libraries ---
print("Installing required libraries...")
!pip install -q deepface

import os
import glob
from deepface import DeepFace

# --- Step 2: Configuration ---
ORIGINAL_IMAGES_DIR = "/content/drive/MyDrive/photo_gen_cloud/phtoto_and_masked/ckh_imgae"
GENERATED_IMAGES_DIR = "/content/drive/MyDrive/flux_models/my_character_lora/generated_images"

# --- Step 3: Sanity Check - Verify DeepFace on an Original Image ---
print("\n--- Running Sanity Check on an Original Image ---")
original_image_paths = sorted(glob.glob(os.path.join(ORIGINAL_IMAGES_DIR, "*.jpg")))
if not original_image_paths:
    original_image_paths = sorted(glob.glob(os.path.join(ORIGINAL_IMAGES_DIR, "*.png")))

if len(original_image_paths) < 2:
    print("Error: Need at least two original images in the directory for a sanity check.")
else:
    try:
        # Compare the first original image with the second one
        img1_path = original_image_paths[0]
        img2_path = original_image_paths[1]
        print(f"Comparing two original images: '{os.path.basename(img1_path)}' and '{os.path.basename(img2_path)}'")

        result = DeepFace.verify(
            img1_path=img1_path,
            img2_path=img2_path,
            detector_backend='mtcnn'
        )
        print(f"✅ Sanity check PASSED. DeepFace is working correctly. Distance: {result['distance']}")
    except Exception as e:
        print(f"❌ Sanity check FAILED. There is a problem with the DeepFace installation or the original images.")
        print(f"   Error details: {e}")


# --- Step 4: Re-run Comparison on Generated Images with Better Error Reporting ---
print("\n--- Retrying Similarity Comparison on Generated Images ---")
saved_image_paths = sorted(glob.glob(os.path.join(GENERATED_IMAGES_DIR, "*.png")))

if not original_image_paths:
     print("Error: Could not find any original images in the directory to compare against.")
elif not saved_image_paths:
     print("Error: Could not find any generated images in the directory.")
else:
    reference_image_path = original_image_paths[0]
    print(f"Using original image '{os.path.basename(reference_image_path)}' as the reference for comparison.")

    for i, gen_path in enumerate(saved_image_paths):
        try:
            result = DeepFace.verify(
                img1_path = gen_path,
                img2_path = reference_image_path,
                detector_backend='mtcnn'
            )
            distance = result['distance']
            print(f"Generated Image {i}: Distance to reference = {distance:.4f} (Should be lower than 0.6)") # Calculated by cosine distance between the embedding vector, the lower the better

        except Exception as e:

            print(f"Warning: Could not process Generated Image {i}. Detailed Error: {e}")

print("\nDiagnostic complete.")

"""# Training Alimama inpainting (Fail due to the mis-matching Controlnet architecture )"""

# @title "Best Effort" Script with the alimama-creative ControlNet
# This script uses the model combination we were debugging.
# It includes all necessary patches and configuration overrides.
# NOTE: This script is expected to fail during the final inference step.

# --- 1. Installation (Requires specific versions) ---
print("✅ Installing required libraries...")
!pip uninstall -y torch torchvision torchaudio diffusers transformers accelerate
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip install -q diffusers==0.33.0 transformers accelerate
!pip install -q opencv-python-headless

# --- 2. Download and Patch Custom Code (Required for this model) ---
print("\n✅ Downloading and patching custom model files...")
# Download
!wget -q -O pipeline_flux_controlnet_inpaint.py https://raw.githubusercontent.com/alimama-creative/FLUX-Controlnet-Inpainting/main/pipeline_flux_controlnet_inpaint.py
!wget -q -O transformer_flux.py https://raw.githubusercontent.com/alimama-creative/FLUX-Controlnet-Inpainting/main/transformer_flux.py
!wget -q -O controlnet_flux.py https://raw.githubusercontent.com/alimama-creative/FLUX-Controlnet-Inpainting/main/controlnet_flux.py

# Apply all necessary patches
try:
    pipeline_path = '/content/pipeline_flux_controlnet_inpaint.py'
    controlnet_path = '/content/controlnet_flux.py'
    transformer_path = '/content/transformer_flux.py'
    # Patch 1: Fix imports in controlnet_flux.py
    with open(controlnet_path, 'r') as f: code = f.read()
    code = code.replace("from diffusers.models.controlnet import BaseOutput, zero_module","from diffusers.utils.outputs import BaseOutput\nfrom diffusers.models.controlnet import zero_module")
    with open(controlnet_path, 'w') as f: f.write(code)
    # Patch 2: Fix attention processor in transformer_flux.py
    with open(transformer_path, 'r') as f: code = f.read()
    code = code.replace("FluxSingleAttnProcessor2_0","FluxAttnProcessor2_0")
    with open(transformer_path, 'w') as f: f.write(code)
    # Patch 3: Fix channel construction in the custom pipeline
    with open(pipeline_path, 'r') as f: code = f.read()
    original_line = "control_image = torch.cat([image_latents, mask], dim=1)"
    corrected_line = "control_image = torch.cat([masked_image_latents, image_latents], dim=1)"
    if original_line in code: code = code.replace(original_line, corrected_line)
    with open(pipeline_path, 'w') as f: f.write(code)
    print("✅ All patches applied successfully.")
except Exception as e: print(f"❌ An error occurred during patching: {e}")

# --- 3. Imports (Must import the custom, patched files) ---
import torch
import importlib
from PIL import Image, ImageDraw
from diffusers.utils import load_image

# Reload modules to ensure patches are used
import pipeline_flux_controlnet_inpaint; importlib.reload(pipeline_flux_controlnet_inpaint)
import transformer_flux; importlib.reload(transformer_flux)
import controlnet_flux; importlib.reload(controlnet_flux)

from pipeline_flux_controlnet_inpaint import FluxControlNetInpaintingPipeline
from transformer_flux import FluxTransformer2DModel
from controlnet_flux import FluxControlNetModel
print("✅ Custom modules imported.")


# --- 4. Load the Components (The complex part) ---
# Unlike the official model, we must load each piece and fix its configuration.
print("✅ Loading the multi-component ControlNet pipeline...")

CONTROLNET_MODEL_ID = "alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Alpha"
BASE_MODEL_ID = "black-forest-labs/FLUX.1-dev"

# Load the ControlNet
controlnet = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=torch.bfloat16)

# Load the Transformer, overriding its config to fix architectural mismatches
transformer = FluxTransformer2DModel.from_pretrained(
    BASE_MODEL_ID,
    subfolder='transformer',
    torch_dtype=torch.bfloat16,
    in_channels=16, # Fix VAE output (16) vs Transformer input (64)
    ignore_mismatched_sizes= True, # Skip loading the incompatible layer
    low_cpu_mem_usage=False # Fix "meta tensor" error

)

# Load the Pipeline, passing in the components and fixing memory issues
pipeline = FluxControlNetInpaintingPipeline.from_pretrained(
    BASE_MODEL_ID,
    controlnet=controlnet,
    transformer=transformer,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=False # Fix "meta tensor" error
)
pipeline.to("cuda")
print("✅ All components loaded and configured.")

# --- 5. Prepare Inputs ---
print("✅ Preparing image and mask...")
image = load_image("/content/drive/MyDrive/photo_gen_cloud/phtoto_and_masked/girl_in_tokyo_street.png").resize((768, 768))

mask = Image.new("L", image.size, 0)
draw = ImageDraw.Draw(mask)
draw.rectangle(((200, 220), (700, 680)), fill=255)

prompt = "A high-resolution photo of a majestic white castle, cinematic"

# --- 6. Run Inference (This step will fail) ---
print("\n❗️ Running inference. This is where the final, unresolvable error occurs.")
try:
    generated_image = pipeline(
        prompt=prompt,
        control_image=image,
        control_mask=mask,
        height=768,
        width=768,
        num_inference_steps=25,
        guidance_scale=4.0
    ).images[0]

    print("✅ Inpainting complete!")
    display(generated_image)

except Exception as e:
    print("\n" + "="*20)
    print("❌ INFERENCE FAILED AS EXPECTED ❌")
    print("="*20)
    print(f"This is the final error we diagnosed, caused by a deep incompatibility in the model's internal logic:\n\n{e}")

"""# Test_FLUX_Fill_LoRa_Training

"""

# Commented out IPython magic to ensure Python compatibility.
# @title 1. Clone Repository and Install Dependencies

print("Cloning Sebastian-Zok/FLUX-Fill-LoRa-Training repository...")
!git clone https://github.com/Sebastian-Zok/FLUX-Fill-LoRa-Training.git /content/flux-fill-lora-training

# %cd /content/flux-fill-lora-training

print("\nInstalling PyTorch, Accelerate, PEFT, and other core libraries...")
# Pin PyTorch and related versions
!pip install -qqq torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121

# Install requirements from the repository's requirements.txt
print("Installing repository specific requirements...")
!pip install -qqq -r requirements.txt

# Install other common useful libraries for diffusion models
!pip install -qqq "accelerate>=0.31.0"
!pip install -qqq "peft>=0.15.0"
!pip install -qqq transformers==4.43.3
!pip install -qqq xformers==0.0.28
!pip install -qqq bitsandbytes==0.43.1
!pip install -qqq huggingface_hub==0.29.0
!pip install -qqq deepface

print("\n--- Verifying crucial library versions ---")
!pip show accelerate
!pip show peft
!pip show torch
print("--------------------------------------\n")

# --- Patching train_dreambooth_inpaint_lora_flux.py ---
print("Applying patch to train_dreambooth_inpaint_lora_flux.py...")
script_path = "/content/flux-fill-lora-training/examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora_flux.py"
try:
    with open(script_path, 'r') as f:
        content = f.read()

    new_content = content.replace(
        'mask = get_mask(pil_image.size, example["image_path"], example["mask_data_path"], 1, False)',
        'mask = get_mask(pil_image.size, example["image_path"], example["mask_data_path"])'
    )

    with open(script_path, 'w') as f:
        f.write(new_content)
    print("✅ Patch applied successfully.")
except Exception as e:
    print(f"❌ Error applying patch: {e}")
    print("Training might fail due to the get_mask() TypeError.")

print("Setup complete. Moving back to /content/")
# %cd /content/

# @title 2. Mount Google Drive and Set Paths

from google.colab import drive
import os

drive.mount('/content/drive')

# @markdown ### **User Configuration (Paths & Prompts):**
# @markdown **Important:** Replace these with your actual paths!

# @markdown Path to your folder containing original training images on Google Drive.
# @markdown Example: /content/drive/MyDrive/my_datasets/my_sks_dog_images
INSTANCE_DATA_DIR = "/content/drive/MyDrive/photo_gen_cloud/phtoto_and_masked/ckh_image" # @param {type:"string"}

# @markdown Path to your folder containing **corresponding binary masks** for the instance images.
# @markdown Masks should be white (255) for the masked area, black (0) for the unmasked area.
# @markdown File names in this directory MUST match those in INSTANCE_DATA_DIR.
# @markdown Example: /content/drive/MyDrive/my_datasets/my_sks_dog_masks
MASKED_DATASET_DIR = "/content/drive/MyDrive/photo_gen_cloud/phtoto_and_masked/masked_ckh_image" # @param {type:"string"}

# @markdown Path where the trained LoRA model will be saved.
# @markdown Example: /content/drive/MyDrive/flux_loras/my_character_inpainting_lora
OUTPUT_DIR = "/content/drive/MyDrive/flux_models/my_character_inpainting_lora_flux_fill_repo" # @param {type:"string"}

# @markdown Your unique trigger word (instance prompt).
# @markdown Example: "a photo of a sks dog"
INSTANCE_PROMPT = "a photo of a sks male celebrity" # @param {type:"string"}

# @markdown Optional: Class prompt for regularization (e.g., "dog", "art style").
# @markdown This helps prevent overfitting and improves generalization.
CLASS_PROMPT = "a sks celebrity" # @param {type:"string"}

# @markdown Create output directory if it doesn't exist
os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"Instance Data Directory: {INSTANCE_DATA_DIR}")
print(f"Masks Data Directory: {MASKED_DATASET_DIR}")
print(f"Output Directory: {OUTPUT_DIR}")
print(f"Instance Prompt: {INSTANCE_PROMPT}")
print(f"Class Prompt: {CLASS_PROMPT}")

if not os.path.exists(INSTANCE_DATA_DIR):
    print("\nWARNING: Instance Data Directory does not exist! Please check your path.")
if not os.path.exists(MASKED_DATASET_DIR):
    print("WARNING: Masks Data Directory does not exist! Please check your path and ensure masks are prepared.")

# Commented out IPython magic to ensure Python compatibility.
# @title 3. Define Training Parameters and Build Command

# @markdown ### **Model and Training Settings:**
MODEL_NAME = "black-forest-labs/FLUX.1-Fill-dev" # @param {type:"string"}
RESOLUTION = 1024 # @param [512, 1024] {type:"raw"}
TRAIN_BATCH_SIZE = 1 # @param {type:"integer"}
GRADIENT_ACCUMULATION_STEPS = 2 # @param {type:"integer"}
MIXED_PRECISION = "bf16" # @param ["no", "fp16", "bf16"]
LEARNING_RATE = "5e-5" # @param {type:"string"}
LR_SCHEDULER = "constant" # @param ["constant", "cosine", "linear", "polynomial"]
LR_WARMUP_STEPS = 0 # @param {type:"integer"}
MAX_TRAIN_STEPS = 1500 # @param {type:"integer"}

# @markdown ### **LoRA Specific Settings:**
LORA_RANK = 32 # @param [8, 16, 32, 64, 128] {type:"raw"}
TRAIN_TEXT_ENCODER = True # @param {type:"boolean"}

# @markdown ### **Memory Optimization and Other Flags:**
GRADIENT_CHECKPOINTING = True # @param {type:"boolean"}
USE_8BIT_ADAM = True # @param {type:"boolean"}

SAVE_INTERVAL = 500 # @param {type:"integer"}
REPORT_TO_WANDB = False # @param {type:"boolean"}

# Construct the command for accelerate launch

class_prompt_arg = f"--class_prompt=\"{CLASS_PROMPT}\"" if CLASS_PROMPT else ""
gradient_checkpointing_arg = "--gradient_checkpointing" if GRADIENT_CHECKPOINTING else ""
use_8bit_adam_arg = "--use_8bit_adam" if USE_8BIT_ADAM else ""
train_text_encoder_arg = "--train_text_encoder" if TRAIN_TEXT_ENCODER else ""
report_to_wandb_arg = "--report_to=wandb" if REPORT_TO_WANDB else ""

# Define the full path to the specific training script
TRAINING_SCRIPT_PATH = "/content/flux-fill-lora-training/examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint_lora_flux.py"

# The actual command string
TRAINING_COMMAND_ARGS = f""" \\
    --mixed_precision="{MIXED_PRECISION}" \\
    {TRAINING_SCRIPT_PATH} \\
    --pretrained_model_name_or_path="{MODEL_NAME}" \\
    --instance_data_dir="{INSTANCE_DATA_DIR}" \\
    --mask_data_dir="{MASKED_DATASET_DIR}" \\
    --output_dir="{OUTPUT_DIR}" \\
    --instance_prompt="{INSTANCE_PROMPT}" \\
    {class_prompt_arg} \\
    --resolution={RESOLUTION} \\
    --train_batch_size={TRAIN_BATCH_SIZE} \\
    --gradient_accumulation_steps={GRADIENT_ACCUMULATION_STEPS} \\
    --learning_rate={LEARNING_RATE} \\
    --lr_scheduler={LR_SCHEDULER} \\
    --lr_warmup_steps={LR_WARMUP_STEPS} \\
    --max_train_steps={MAX_TRAIN_STEPS} \\
    --rank={LORA_RANK} \\
    --checkpointing_steps={SAVE_INTERVAL} \\
    {gradient_checkpointing_arg} \\
    {use_8bit_adam_arg} \\
    {train_text_encoder_arg} \\
    {report_to_wandb_arg}
"""
# This is the actual command that will be executed in Cell 5.

FINAL_EXECUTION_COMMAND = f"""
# %cd /content/flux-fill-lora-training
accelerate launch {TRAINING_COMMAND_ARGS}
# %cd /content/
"""


print("Corrected training command constructed. Review it below:")
print(FINAL_EXECUTION_COMMAND)

# @title 4. Log in to Hugging Face

from huggingface_hub import login

login("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")

print("Login successful!")

# @title 5. Run the Training

if REPORT_TO_WANDB:
    import wandb
    wandb.login()

print("Starting training...")

!{FINAL_EXECUTION_COMMAND}

print("\nTraining completed! Your LoRA model should be saved in:")
print(OUTPUT_DIR)

# @title 6. Generate an Inpainted Image with the Trained LoRA

import torch
from diffusers import FluxFillPipeline
from diffusers.utils import load_image
from PIL import Image, ImageDraw
import os
from IPython.display import display

# --- Configuration ---
# This base model MUST match the one you used for training in the previous cell.
MODEL_ID = "black-forest-labs/FLUX.1-Fill-dev"

# This should be the same as the OUTPUT_DIR from your training step.
LORA_PATH = "/content/drive/MyDrive/flux_models/my_character_inpainting_lora_flux_fill_repo"

# The image you want to inpaint.
INPUT_IMAGE_PATH = "/content/drive/MyDrive/photo_gen_cloud/phtoto_and_masked/girl_in_tokyo_street.png"
# The corresponding mask for the image.
MASK_IMAGE_PATH = "/content/drive/MyDrive/photo_gen_cloud/phtoto_and_masked/masked_3_girl_in_tokyo_street.jpg"

# Where to save the final result.
SAVE_DIR = os.path.join(LORA_PATH, "final_results")
os.makedirs(SAVE_DIR, exist_ok=True)

# The prompt describing what you want to generate in the masked area.
# Remember to use the trigger word from your training (e.g., "sks dog").
PROMPT = "a high-resolution photo of a ckh celebrity, in realistic style, Ivy League Cut hair, standing next to the bike girl"

# --- Load Pipeline and LoRA ---
print(f"Loading base inpainting model: {MODEL_ID}")
pipeline = FluxFillPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16)

print(f"Loading your trained LoRA weights from: {LORA_PATH}")
pipeline.load_lora_weights(LORA_PATH)
pipeline.to("cuda")

# --- Prepare Image and Mask ---
print(f"Loading and preparing images...")
if not os.path.exists(INPUT_IMAGE_PATH) or not os.path.exists(MASK_IMAGE_PATH):
    print(f"❌ Error: Image or Mask not found. Please check your paths.")
else:
    # FLUX models work best at 1024x1024 resolution
    image = load_image(INPUT_IMAGE_PATH).resize((1024, 1024))
    mask = load_image(MASK_IMAGE_PATH).resize((1024, 1024))

    # --- Run Inference ---
    print(f"\nRunning inpainting with prompt: '{PROMPT}'")
    generator = torch.Generator(device="cuda").manual_seed(42)

    generated_image = pipeline(
        prompt=PROMPT,
        image=image,
        mask_image=mask,
        num_inference_steps=50,
        guidance_scale=3.55, # You can experiment with this value
        generator=generator,
    ).images[0]

    # --- Save and Display Result ---
    output_image_path = os.path.join(SAVE_DIR, "final_inpainted_image.png")
    generated_image.save(output_image_path)
    print(f"✅ Inpainting complete! Saved to: {output_image_path}")

    # Display the original, mask, and result side-by-side
    final_display = Image.new("RGB", (image.width * 3, image.height))
    final_display.paste(image, (0, 0))
    final_display.paste(mask, (image.width, 0))
    final_display.paste(generated_image, (image.width * 2, 0))

    print("\nFrom left to right: Original Image, Mask, Final Inpainted Result")
    display(final_display)